{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    #weather Another #fakenews video @CNN Anderson...\n",
       "1    Staring Down Hurricane Florence via NASA https...\n",
       "2    #hurricane #florence http:// tra.one/tcphurFlo...\n",
       "3    Hurricane Florence is still moving through the...\n",
       "4    Houston is collecting donations for Hurricane ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "df = pd.read_csv('C:\\\\Ashish\\\\Project\\\\dataset\\\\trainset1.csv')#,nrows = 1500)\n",
    "df.text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['original_text'] = df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1009\n",
       "2     588\n",
       "3     307\n",
       "4     118\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #SAMPLING\n",
    "# from sklearn.utils import resample\n",
    "\n",
    "# # Separate majority and minority classes\n",
    "# df_majority = df[df.category==1]\n",
    "# df_minority = df[df.category!=1]\n",
    "\n",
    "# # Downsample majority class\n",
    "# df_majority_downsampled = resample(df_majority, \n",
    "#                                  replace=False,    # sample without replacement\n",
    "#                                  n_samples=1009,     # to match minority class\n",
    "#                                  random_state=123) # reproducible results\n",
    " \n",
    "# # Combine minority class with downsampled majority class\n",
    "# df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "\n",
    "# df = df_downsampled\n",
    "\n",
    "# # Display new class counts\n",
    "# df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "    return input_txt  \n",
    "\n",
    "# remove twitter handles (@user)\n",
    "df['text'] = np.vectorize(remove_pattern)(df['text'], \"@[\\w]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO - REMOVE ASCII EXTENDED\n",
    "for i in range(len(df)):\n",
    "    tweet = df.iloc[i,df.columns.get_loc('text')]\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r\"\\S+\\/.\\S+ *\\S+|.\\S+html|\\S+-\\S+|\\d*\\/\\d+|\\d+|\\S+%\\S+|\\S+:\\S*|\\S+=\\S+|.#\\S+\", \"\", tweet)\n",
    "    tweet = tweet.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "    df.iloc[i,df.columns.get_loc('text')] = tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "stop_words = set(stopwords.words('english'))\n",
    "for i in range(len(df)):\n",
    "    tweet = df.iloc[i,df.columns.get_loc('text')]\n",
    "    word_tokens = word_tokenize(tweet) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    filtered_sentence = [] \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w) \n",
    "    tweet = ' '.join(filtered_sentence)\n",
    "    df.iloc[i,df.columns.get_loc('text')] = tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_words = ['via','rt','fav','…','am','et','pm','n\\'t','y\\'all']\n",
    "for i in range(len(df)):\n",
    "    tweet = df.iloc[i,df.columns.get_loc('text')]\n",
    "    querywords = tweet.split()\n",
    "    resultwords  = [word for word in querywords if word.lower() not in custom_words]\n",
    "    result = ' '.join(resultwords)\n",
    "    df.iloc[i,df.columns.get_loc('text')] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "remove = string.punctuation + \".‘’\\''“”°…-—––•・®.:#\"\n",
    "for i in range(len(df)):\n",
    "    tweet = df.iloc[i,df.columns.get_loc('text')]\n",
    "    tweet = ' '.join(word.strip(remove) for word in tweet.split())\n",
    "    tweet = tweet.strip()\n",
    "    df.iloc[i,df.columns.get_loc('text')] = tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize with POS Tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# 1. Init Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for i in range(len(df)):\n",
    "    tweet = df.iloc[i,df.columns.get_loc('text')]\n",
    "    tweet =([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(tweet)])\n",
    "    tweet = ' '.join(tweet)\n",
    "    df.iloc[i,df.columns.get_loc('text')] = tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REVIEW NEEDED\n",
    "import nltk\n",
    "words = set(nltk.corpus.words.words())\n",
    "for i in range(len(df)):\n",
    "    tweet = df.iloc[i,df.columns.get_loc('text')]\n",
    "    tweet = ' '.join(word for word in tweet.split() if len(word)>3)\n",
    "    df.iloc[i,df.columns.get_loc('text')] = tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = df['text'].str.split().str.len()\n",
    "# df = df[~(count < 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check number of words and verify absence of sentences with length less than 3\n",
    "# for index, row in df.iterrows():\n",
    "#     print(len(row.text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, row in df.iterrows():\n",
    "#     print(row.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('C:\\\\Ashish\\\\Project\\\\dataset\\\\clean-trainset1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------END--OF--PRE-PROCESSING----------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[['text','category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2[pd.notnull(df2['text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weather another video anderson cooper hype flo...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>star hurricane florence nasa</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hurricane florence still move atlantic morning...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>houston collect donation hurricane florence vi...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>still collect supply victim hurricane florence...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>whats surprising hurricane florence thrive abn...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>denim bunny hurricane florence hair</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>listen zello cajun navy hurricane florence gro...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>grandmama know wrong talk hurricane aint take ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hurricaneupdate euro</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>receive update city bowie part alert bowie mes...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>five death record thus regard hurricane floren...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hurricane florence move towards greenville loc...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rain arrive earlier expect blargh feel like wa...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>even though hate love fema never want fail pra...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>happen bern people folk evacuate town poverty ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>hurricaneflorence experience symptom cabin fev...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>typhoon ready philippine category typhoon powe...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>tremendously tremendously</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>year month track today track also head towards...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>fuck hurricane florence ruin everything</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>blonde hurricane florence hayley</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>predict catastrophic flood hurricane know home...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>help hurricane florence relief effort donate t...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>wind wave obey respecter race bless lamb judge...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>hear donald trump come visit north carolina ne...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>muscogee creek nation team prepares leave hurr...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>hurricane florence make landfall north carolin...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>disaster distress helpline provide immediate c...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>need track impend hurricane florence head caro...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>hurricane florence produce foot wave nears car...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>narrative cult</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>heart others lose life hurricane florence</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>half people photo planning evacuate barrier is...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>hurricane florence survivalist notice people t...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>yass queen fuck shit hurricane florence expect...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>state emergency declare ahead</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>north carolina coast look north away florence ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>weve always fresh cinnamon roll blueberry roll...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>hurricane florence tropical cyclone update atl...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  category  category_id\n",
       "0   weather another video anderson cooper hype flo...         2            0\n",
       "1                        star hurricane florence nasa         2            0\n",
       "3   hurricane florence still move atlantic morning...         1            1\n",
       "4   houston collect donation hurricane florence vi...         3            2\n",
       "6   still collect supply victim hurricane florence...         3            2\n",
       "7   whats surprising hurricane florence thrive abn...         2            0\n",
       "8                 denim bunny hurricane florence hair         1            1\n",
       "9   listen zello cajun navy hurricane florence gro...         3            2\n",
       "10  grandmama know wrong talk hurricane aint take ...         1            1\n",
       "11                               hurricaneupdate euro         2            0\n",
       "12  receive update city bowie part alert bowie mes...         2            0\n",
       "13  five death record thus regard hurricane floren...         2            0\n",
       "14  hurricane florence move towards greenville loc...         1            1\n",
       "15  rain arrive earlier expect blargh feel like wa...         1            1\n",
       "16  even though hate love fema never want fail pra...         3            2\n",
       "17  happen bern people folk evacuate town poverty ...         1            1\n",
       "18  hurricaneflorence experience symptom cabin fev...         2            0\n",
       "19  typhoon ready philippine category typhoon powe...         1            1\n",
       "20                          tremendously tremendously         1            1\n",
       "21  year month track today track also head towards...         1            1\n",
       "22            fuck hurricane florence ruin everything         2            0\n",
       "23                   blonde hurricane florence hayley         1            1\n",
       "24  predict catastrophic flood hurricane know home...         1            1\n",
       "25  help hurricane florence relief effort donate t...         3            2\n",
       "26  wind wave obey respecter race bless lamb judge...         2            0\n",
       "27  hear donald trump come visit north carolina ne...         2            0\n",
       "28  muscogee creek nation team prepares leave hurr...         1            1\n",
       "29  hurricane florence make landfall north carolin...         2            0\n",
       "30  disaster distress helpline provide immediate c...         3            2\n",
       "31  need track impend hurricane florence head caro...         1            1\n",
       "32  hurricane florence produce foot wave nears car...         2            0\n",
       "33                                     narrative cult         4            3\n",
       "34          heart others lose life hurricane florence         2            0\n",
       "35  half people photo planning evacuate barrier is...         1            1\n",
       "36  hurricane florence survivalist notice people t...         1            1\n",
       "37  yass queen fuck shit hurricane florence expect...         1            1\n",
       "38                      state emergency declare ahead         1            1\n",
       "39  north carolina coast look north away florence ...         1            1\n",
       "40  weve always fresh cinnamon roll blueberry roll...         4            3\n",
       "41  hurricane florence tropical cyclone update atl...         1            1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['category_id'] = df2['category'].factorize()[0]\n",
    "category_id_df = df2[['category', 'category_id']].drop_duplicates().sort_values('category_id')\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['category_id', 'category']].values)\n",
    "df2.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# fig = plt.figure(figsize=(8,6))\n",
    "# df2.groupby('category').text.count().plot.bar(ylim=0)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# tfidf = TfidfVectorizer(sublinear_tf=True, min_df=0, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "# # tfidf = TfidfVectorizer()\n",
    "# features = tfidf.fit_transform(df2.text).toarray()\n",
    "# labels = df2.category_id\n",
    "# features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import chi2\n",
    "\n",
    "# N = 2\n",
    "# for category, category_id in sorted(category_to_id.items()):\n",
    "#   features_chi2 = chi2(features, labels == category_id)\n",
    "#   indices = np.argsort(features_chi2[0])\n",
    "#   feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "#   unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "#   bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "#   print(\"# '{}':\".format(category))\n",
    "#   print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n",
    "#   print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df2['text'], df2['category'], random_state = 0)\n",
    "# count_vect = CountVectorizer()\n",
    "# X_train_counts = count_vect.fit_transform(X_train)\n",
    "# tfidf_transformer = TfidfTransformer()\n",
    "# X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Test one sentence\n",
    "# print(clf.predict(count_vect.transform([\"help donate hurricane\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.svm import LinearSVC\n",
    "\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# models = [\n",
    "#     LinearSVC(),\n",
    "#     MultinomialNB(),\n",
    "#     LogisticRegression(random_state=0),\n",
    "# ]\n",
    "# CV = 5\n",
    "# cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "# entries = []\n",
    "# for model in models:\n",
    "#   model_name = model.__class__.__name__\n",
    "#   accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n",
    "#   for fold_idx, accuracy in enumerate(accuracies):\n",
    "#     entries.append((model_name, fold_idx, accuracy))\n",
    "# cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "\n",
    "# import seaborn as sns\n",
    "\n",
    "# sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "# sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "#               size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_df.groupby('model_name').accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LinearSVC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-de9cb67967dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LinearSVC' is not defined"
     ]
    }
   ],
   "source": [
    "model = LinearSVC(random_state=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.25, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=category_id_df.category.values, yticklabels=category_id_df.category.values)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "pd.set_option('display.max_rows', 500)\n",
    "for predicted in category_id_df.category_id:\n",
    "  for actual in category_id_df.category_id:\n",
    "    if predicted != actual and conf_mat[actual, predicted] >= 10:\n",
    "      print(\"'{}' predicted as '{}' : {} examples.\".format(id_to_category[actual], id_to_category[predicted], conf_mat[actual, predicted]))\n",
    "      display(df.loc[indices_test[(y_test == actual) & (y_pred == predicted)]][['category', 'text']])\n",
    "      print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    985\n",
       "2    580\n",
       "3    304\n",
       "4     84\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'astronaut take hurricane florence photo truly chill'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.text[286]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'2' predicted as '1' : 35 examples.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1322</th>\n",
       "      <td>2</td>\n",
       "      <td>watch hurricane florence flood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>2</td>\n",
       "      <td>hurricane florence hurricane warning issue par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1483</th>\n",
       "      <td>2</td>\n",
       "      <td>people like gore make money climate change imm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>2</td>\n",
       "      <td>science hurricane florence mile large rain tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>2</td>\n",
       "      <td>tower american flag kevin becomes hurricane fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721</th>\n",
       "      <td>2</td>\n",
       "      <td>hurricane florence intensified fast still know...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1639</th>\n",
       "      <td>2</td>\n",
       "      <td>direct view hurricane florence come already ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>2</td>\n",
       "      <td>track hurricane florence path</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2</td>\n",
       "      <td>black people make everything joke twitter page...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>2</td>\n",
       "      <td>witness hurricane florence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1608</th>\n",
       "      <td>2</td>\n",
       "      <td>still long line driver shell shellmore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>2</td>\n",
       "      <td>hurricane florence evacuation cause traffic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>2</td>\n",
       "      <td>ughhhhhhh ford horizon demo download large hur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>2</td>\n",
       "      <td>trash mile away</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>2</td>\n",
       "      <td>hurricane florence unusual extreme worsen clim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>2</td>\n",
       "      <td>hurricaneflorence yesterday tree company compa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>2</td>\n",
       "      <td>population vulnerable climate change lock path...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>2</td>\n",
       "      <td>weaken hurricane florence approach north carol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>2</td>\n",
       "      <td>hurricane florence million people told evacuat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>2</td>\n",
       "      <td>last night durham week people gather justice e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1325</th>\n",
       "      <td>2</td>\n",
       "      <td>hurricane florence coverage show medium virali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>2</td>\n",
       "      <td>hurricaneflorence fight square</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>2</td>\n",
       "      <td>hurricane make right towards south border watc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>2</td>\n",
       "      <td>official voice stay connect tune garrison face...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>2</td>\n",
       "      <td>hahahahahahah remnant hurricane florence news ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>2</td>\n",
       "      <td>congressional sophomoric tweet make light hurr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>2</td>\n",
       "      <td>trump brag tremendous accolade response hurric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655</th>\n",
       "      <td>2</td>\n",
       "      <td>spring maryland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>2</td>\n",
       "      <td>espectacular imagen sargazos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>2</td>\n",
       "      <td>thought hurricane florence aint shit fucker ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>2</td>\n",
       "      <td>legacyinsgrp insurance_day hurricane florence ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>2</td>\n",
       "      <td>hurricane florence trudge across carolina subm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1691</th>\n",
       "      <td>2</td>\n",
       "      <td>eleven people hurricane florence wild weather ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>2</td>\n",
       "      <td>mean like medium story story hurricane florenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>2</td>\n",
       "      <td>hurricane florence list schedule change athlet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      category                                               text\n",
       "1322         2                     watch hurricane florence flood\n",
       "1579         2  hurricane florence hurricane warning issue par...\n",
       "1483         2  people like gore make money climate change imm...\n",
       "447          2  science hurricane florence mile large rain tha...\n",
       "357          2  tower american flag kevin becomes hurricane fl...\n",
       "1721         2  hurricane florence intensified fast still know...\n",
       "1639         2  direct view hurricane florence come already ri...\n",
       "154          2                      track hurricane florence path\n",
       "83           2  black people make everything joke twitter page...\n",
       "1197         2                         witness hurricane florence\n",
       "1608         2             still long line driver shell shellmore\n",
       "439          2        hurricane florence evacuation cause traffic\n",
       "265          2  ughhhhhhh ford horizon demo download large hur...\n",
       "145          2                                    trash mile away\n",
       "1115         2  hurricane florence unusual extreme worsen clim...\n",
       "1068         2  hurricaneflorence yesterday tree company compa...\n",
       "323          2  population vulnerable climate change lock path...\n",
       "299          2  weaken hurricane florence approach north carol...\n",
       "130          2  hurricane florence million people told evacuat...\n",
       "168          2  last night durham week people gather justice e...\n",
       "1325         2  hurricane florence coverage show medium virali...\n",
       "1051         2                     hurricaneflorence fight square\n",
       "1150         2  hurricane make right towards south border watc...\n",
       "934          2  official voice stay connect tune garrison face...\n",
       "1202         2  hahahahahahah remnant hurricane florence news ...\n",
       "799          2  congressional sophomoric tweet make light hurr...\n",
       "935          2  trump brag tremendous accolade response hurric...\n",
       "1655         2                                    spring maryland\n",
       "126          2                       espectacular imagen sargazos\n",
       "1990         2  thought hurricane florence aint shit fucker ma...\n",
       "922          2  legacyinsgrp insurance_day hurricane florence ...\n",
       "211          2  hurricane florence trudge across carolina subm...\n",
       "1691         2  eleven people hurricane florence wild weather ...\n",
       "1169         2  mean like medium story story hurricane florenc...\n",
       "973          2  hurricane florence list schedule change athlet..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'3' predicted as '1' : 15 examples.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1381</th>\n",
       "      <td>3</td>\n",
       "      <td>fkec sends crew assist hurricane florence rest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>3</td>\n",
       "      <td>discuss hurricane florence president donald tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131</th>\n",
       "      <td>3</td>\n",
       "      <td>watch news cant seem wrap mind around donate w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>3</td>\n",
       "      <td>affected think hope love stay safe continue po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>3</td>\n",
       "      <td>waffle house index play role fema hurricane re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>3</td>\n",
       "      <td>thanks costal federal credit union week wait d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>3</td>\n",
       "      <td>please link purchase proceeds family need hurr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>3</td>\n",
       "      <td>repost train cross disaster worker deployed en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>3</td>\n",
       "      <td>bragging right student lead food drive collect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>3</td>\n",
       "      <td>amtrak begin resume service hurricane florence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>3</td>\n",
       "      <td>although convict prisoner forfeit right libert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>3</td>\n",
       "      <td>hurricane florence barrel towards always quarter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>3</td>\n",
       "      <td>market game hurricane florence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>3</td>\n",
       "      <td>hurricane florence cant keep away grille away ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>3</td>\n",
       "      <td>hurricane florence airline begin waive change</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      category                                               text\n",
       "1381         3  fkec sends crew assist hurricane florence rest...\n",
       "1239         3  discuss hurricane florence president donald tr...\n",
       "1131         3  watch news cant seem wrap mind around donate w...\n",
       "1063         3  affected think hope love stay safe continue po...\n",
       "787          3  waffle house index play role fema hurricane re...\n",
       "731          3  thanks costal federal credit union week wait d...\n",
       "1769         3  please link purchase proceeds family need hurr...\n",
       "930          3  repost train cross disaster worker deployed en...\n",
       "1398         3  bragging right student lead food drive collect...\n",
       "1586         3     amtrak begin resume service hurricane florence\n",
       "509          3  although convict prisoner forfeit right libert...\n",
       "780          3   hurricane florence barrel towards always quarter\n",
       "1749         3                     market game hurricane florence\n",
       "388          3  hurricane florence cant keep away grille away ...\n",
       "361          3      hurricane florence airline begin waive change"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'1' predicted as '2' : 47 examples.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>1</td>\n",
       "      <td>problem solve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>1</td>\n",
       "      <td>south carolina could suffer devastate flood ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>1</td>\n",
       "      <td>preparation glass gondola remove skywheel myrt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>1</td>\n",
       "      <td>hurricane update outer band rain approach coas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>1</td>\n",
       "      <td>stormtracker north carolina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1</td>\n",
       "      <td>hurricane florence first major hurricane seaso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>1</td>\n",
       "      <td>hurricane florence this really scare lead fore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>1</td>\n",
       "      <td>confidence remains high potential impact like ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>hurricane florence tropical cyclone update atl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>1</td>\n",
       "      <td>dare someone loot consider loot company youre ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>1</td>\n",
       "      <td>hurricane battling analysis convection able re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>1</td>\n",
       "      <td>know would never</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>1</td>\n",
       "      <td>flood threat likely slow along coastline carol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1924</th>\n",
       "      <td>1</td>\n",
       "      <td>host speaks week episode accuweather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>1</td>\n",
       "      <td>laurel park reschedule saturday stake race thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>1</td>\n",
       "      <td>right hope come hurricane florence casually sw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>1</td>\n",
       "      <td>monster hurricane florence nears carolina coast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>1</td>\n",
       "      <td>buck profit type video hurricane florence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>1</td>\n",
       "      <td>hurricane florence prep update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>1</td>\n",
       "      <td>pansy hurricane name florence stop like work s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1</td>\n",
       "      <td>safety concern result hurricane florence dream...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1411</th>\n",
       "      <td>1</td>\n",
       "      <td>unfortunately performance weekend johnson city...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1</td>\n",
       "      <td>hurricane florence world mark dixon focus conc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>1</td>\n",
       "      <td>junk newspaper blame trump hurricane florence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>1</td>\n",
       "      <td>hurricane florence list nuclear power plant st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>1</td>\n",
       "      <td>hurricane florence target carolina flood destr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060</th>\n",
       "      <td>1</td>\n",
       "      <td>point future leader implies hurricane florence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>1</td>\n",
       "      <td>operational surprisingly come line euro model ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>1</td>\n",
       "      <td>lesson cover hurricane florence lagoon north c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>1</td>\n",
       "      <td>want like hurricane florence want pound nonstop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>1</td>\n",
       "      <td>hurricane florence nightmare scenario could na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1</td>\n",
       "      <td>california firefighter head east hurricane flo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>1</td>\n",
       "      <td>hurricaneflorence make landfall week east coas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>1</td>\n",
       "      <td>hurricane florence move slowly southern north ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>1</td>\n",
       "      <td>hurricane florence abroad mean hurricane firenze</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681</th>\n",
       "      <td>1</td>\n",
       "      <td>extreme liberal editorial board wait president...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>1</td>\n",
       "      <td>look like hurricane florence primarily republi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1</td>\n",
       "      <td>late diario world vision thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>1</td>\n",
       "      <td>hurricane florence close carolina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>1</td>\n",
       "      <td>hurricane florence make direct georgia south c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>1</td>\n",
       "      <td>outer band come ashore north carolina tornado ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>1</td>\n",
       "      <td>hurricane locate wind crawl west pressure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>1</td>\n",
       "      <td>playoff game north carolina courage chicago st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>1</td>\n",
       "      <td>hurricaneflorence come grid crosshairs reactor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>1</td>\n",
       "      <td>case miss yesterday landes letter good hurrica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>1</td>\n",
       "      <td>nuclear facility coast dont forget take nuclea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>1</td>\n",
       "      <td>hurricane florence storm path shift million co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      category                                               text\n",
       "662          1                                      problem solve\n",
       "699          1  south carolina could suffer devastate flood ev...\n",
       "1176         1  preparation glass gondola remove skywheel myrt...\n",
       "1461         1  hurricane update outer band rain approach coas...\n",
       "1732         1                        stormtracker north carolina\n",
       "71           1  hurricane florence first major hurricane seaso...\n",
       "1374         1  hurricane florence this really scare lead fore...\n",
       "1932         1  confidence remains high potential impact like ...\n",
       "41           1  hurricane florence tropical cyclone update atl...\n",
       "216          1  dare someone loot consider loot company youre ...\n",
       "1465         1  hurricane battling analysis convection able re...\n",
       "1540         1                                   know would never\n",
       "837          1  flood threat likely slow along coastline carol...\n",
       "1924         1               host speaks week episode accuweather\n",
       "1793         1  laurel park reschedule saturday stake race thr...\n",
       "1416         1  right hope come hurricane florence casually sw...\n",
       "376          1    monster hurricane florence nears carolina coast\n",
       "1061         1          buck profit type video hurricane florence\n",
       "574          1                     hurricane florence prep update\n",
       "747          1  pansy hurricane name florence stop like work s...\n",
       "1997         1  safety concern result hurricane florence dream...\n",
       "1411         1  unfortunately performance weekend johnson city...\n",
       "63           1  hurricane florence world mark dixon focus conc...\n",
       "144          1      junk newspaper blame trump hurricane florence\n",
       "806          1  hurricane florence list nuclear power plant st...\n",
       "1695         1  hurricane florence target carolina flood destr...\n",
       "1060         1  point future leader implies hurricane florence...\n",
       "707          1  operational surprisingly come line euro model ...\n",
       "563          1  lesson cover hurricane florence lagoon north c...\n",
       "1018         1    want like hurricane florence want pound nonstop\n",
       "624          1  hurricane florence nightmare scenario could na...\n",
       "417          1  california firefighter head east hurricane flo...\n",
       "607          1  hurricaneflorence make landfall week east coas...\n",
       "230          1  hurricane florence move slowly southern north ...\n",
       "914          1   hurricane florence abroad mean hurricane firenze\n",
       "1681         1  extreme liberal editorial board wait president...\n",
       "1276         1  look like hurricane florence primarily republi...\n",
       "88           1                    late diario world vision thanks\n",
       "1067         1                  hurricane florence close carolina\n",
       "192          1  hurricane florence make direct georgia south c...\n",
       "1952         1  outer band come ashore north carolina tornado ...\n",
       "1964         1          hurricane locate wind crawl west pressure\n",
       "621          1  playoff game north carolina courage chicago st...\n",
       "532          1  hurricaneflorence come grid crosshairs reactor...\n",
       "796          1  case miss yesterday landes letter good hurrica...\n",
       "775          1  nuclear facility coast dont forget take nuclea...\n",
       "1488         1  hurricane florence storm path shift million co..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'3' predicted as '2' : 12 examples.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>3</td>\n",
       "      <td>response revive flood remember flood read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>3</td>\n",
       "      <td>imagine boarding leave home coast know come ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>3</td>\n",
       "      <td>pewaukee organization save life animal left be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>3</td>\n",
       "      <td>president donald trump administration provide ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>3</td>\n",
       "      <td>glad recognition numerous outlet like name reg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>3</td>\n",
       "      <td>raise little hour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587</th>\n",
       "      <td>3</td>\n",
       "      <td>reunite impact lose found poison helpline emer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>3</td>\n",
       "      <td>trump hurricane florence have good time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>3</td>\n",
       "      <td>michael jordan grow play high school basketbal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>3</td>\n",
       "      <td>cellular step give customer signal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>3</td>\n",
       "      <td>friend donate gofundme myrtle beach stand fron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>3</td>\n",
       "      <td>hurricane hunter hurricane florence tech chris...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      category                                               text\n",
       "758          3          response revive flood remember flood read\n",
       "1144         3  imagine boarding leave home coast know come ba...\n",
       "959          3  pewaukee organization save life animal left be...\n",
       "1092         3  president donald trump administration provide ...\n",
       "1020         3  glad recognition numerous outlet like name reg...\n",
       "133          3                                  raise little hour\n",
       "1587         3  reunite impact lose found poison helpline emer...\n",
       "1137         3            trump hurricane florence have good time\n",
       "660          3  michael jordan grow play high school basketbal...\n",
       "692          3                 cellular step give customer signal\n",
       "178          3  friend donate gofundme myrtle beach stand fron...\n",
       "1073         3  hurricane hunter hurricane florence tech chris..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "for predicted in category_id_df.category_id:\n",
    "  for actual in category_id_df.category_id:\n",
    "    if predicted != actual and conf_mat[actual, predicted] >= 10:\n",
    "      print(\"'{}' predicted as '{}' : {} examples.\".format(id_to_category[actual], id_to_category[predicted], conf_mat[actual, predicted]))\n",
    "      display(df2.loc[indices_test[(y_test == actual) & (y_pred == predicted)]][['category', 'text']])\n",
    "      print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.62      0.64      0.63       151\n",
      "           2       0.60      0.71      0.65       137\n",
      "           3       0.80      0.66      0.72        79\n",
      "           4       0.80      0.20      0.32        20\n",
      "\n",
      "   micro avg       0.64      0.64      0.64       387\n",
      "   macro avg       0.70      0.55      0.58       387\n",
      "weighted avg       0.66      0.64      0.64       387\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def to_str(var):\n",
    "    return str(list(np.reshape(np.asarray(var), (1, np.size(var)))[0]))[1:-1]\n",
    "\n",
    "cat = (df2['category'].unique())\n",
    "l = []\n",
    "\n",
    "for element in cat:\n",
    "    l.append('' + to_str(element))\n",
    "print(metrics.classification_report(y_test, y_pred, target_names = l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    weather another video anderson cooper hype flo...\n",
       "1                         star hurricane florence nasa\n",
       "2    hurricane florence still move atlantic morning...\n",
       "3    houston collect donation hurricane florence vi...\n",
       "4    still collect supply victim hurricane florence...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "train_df = pd.read_csv('C:\\\\Ashish\\\\Project\\\\dataset\\\\clean-trainset1.csv')#,nrows = 1500)\n",
    "train_df.text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df['category_id'] = df['category'].factorize()[0]\n",
    "# category_id_df = df[['category', 'category_id']].drop_duplicates().sort_values('category_id')\n",
    "# category_to_id = dict(category_id_df.values)\n",
    "# id_to_category = dict(category_id_df[['category_id', 'category']].values)\n",
    "# df.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "df.groupby('category').text.count().plot.bar(ylim=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns \n",
    "\n",
    "def train(classifier, X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.08, random_state=33)\n",
    " \n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(\"Accuracy: %s\" % classifier.score(X_test, y_test))\n",
    "    \n",
    "#     from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#     conf_mat = confusion_matrix(y_test, y_pred)\n",
    "#     fig, ax = plt.subplots(figsize=(10,10))\n",
    "#     sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "#                 xticklabels=category_id_df.category.values, yticklabels=category_id_df.category.values)\n",
    "#     plt.ylabel('Actual')\n",
    "#     plt.xlabel('Predicted')\n",
    "#     plt.show()\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7579617834394905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=...      vocabulary=None)), ('classifier', MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "trial = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english'),\n",
    "                             min_df=5,norm='l2')),\n",
    "    ('classifier', MultinomialNB(alpha=0.30)),\n",
    "])\n",
    " \n",
    "train(trial, train_df.text, train_df.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------Classifier trained---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict values (Assume pre processing done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfx = pd.read_csv('C:\\\\Ashish\\\\Project\\\\dataset\\\\clean-trainset1.csv')#,nrows = 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df.text\n",
    "# y = df.category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_categories = trial.predict(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'] = pd.Series(predicted_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# qqq = pd.Series(predicted_categories)\n",
    "# type(qqq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate dataset for IRMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:\\\\Ashish\\\\Project\\\\dataset\\\\irma-large.csv')#,nrows = 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'tweet_url', 'created_at', 'parsed_created_at',\n",
       "       'user_screen_name', 'text', 'tweet_type', 'coordinates', 'hashtags',\n",
       "       'media', 'urls', 'favorite_count', 'in_reply_to_screen_name',\n",
       "       'in_reply_to_status_id', 'in_reply_to_user_id', 'lang', 'place',\n",
       "       'possibly_sensitive', 'retweet_count', 'retweet_or_quote_id',\n",
       "       'retweet_or_quote_screen_name', 'retweet_or_quote_user_id', 'source',\n",
       "       'user_id', 'user_created_at', 'user_default_profile_image',\n",
       "       'user_description', 'user_favourites_count', 'user_followers_count',\n",
       "       'user_friends_count', 'user_listed_count', 'user_location', 'user_name',\n",
       "       'user_statuses_count', 'user_time_zone', 'user_urls', 'user_verified'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, row in df.iterrows():\n",
    "    print(type(row.coordinate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My favorite #tabebuia tree continues on its road to recovery from damage in #HurricaneIrma here in #Naples in #SWFlorida … https://t.co/mrqXM2VRuF … and all my #photos from #Naples https://t.co/XzGnQ6AdFL'"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = type(df.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = x == str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = []\n",
    "for index, row in df.iterrows():\n",
    "    if type(row.coordinates) == str:\n",
    "        store.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-310-1d6223690bb8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdata_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstore\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ashis\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1499\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1500\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1502\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ashis\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   2228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2229\u001b[0m             \u001b[1;31m# validate the location\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2230\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2232\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ashis\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   2137\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2138\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2139\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "data_index = 0\n",
    "for i in store:\n",
    "    data.append((data.iloc[i]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1151068"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store[3949]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'tweet_url', 'created_at', 'parsed_created_at',\n",
       "       'user_screen_name', 'text', 'tweet_type', 'coordinates', 'hashtags',\n",
       "       'media', 'urls', 'favorite_count', 'in_reply_to_screen_name',\n",
       "       'in_reply_to_status_id', 'in_reply_to_user_id', 'lang', 'place',\n",
       "       'possibly_sensitive', 'retweet_count', 'retweet_or_quote_id',\n",
       "       'retweet_or_quote_screen_name', 'retweet_or_quote_user_id', 'source',\n",
       "       'user_id', 'user_created_at', 'user_default_profile_image',\n",
       "       'user_description', 'user_favourites_count', 'user_followers_count',\n",
       "       'user_friends_count', 'user_listed_count', 'user_location', 'user_name',\n",
       "       'user_statuses_count', 'user_time_zone', 'user_urls', 'user_verified'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[['created_at','text','coordinates']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "for i in store:\n",
    "    d.append(data.created_at.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = []\n",
    "for i in store:\n",
    "    t.append(data.text.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = []\n",
    "for i in store:\n",
    "    c.append(data.coordinates.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.DataFrame(list(zip(d, c, t)),\n",
    "              columns=['date','coordinates', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thu Jan 10 18:37:23 +0000 2019</td>\n",
       "      <td>-80.192000 25.775200</td>\n",
       "      <td>#publicadjuster #wefightforyou #hurricaneirma ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wed Jan 02 15:13:57 +0000 2019</td>\n",
       "      <td>-81.592912 29.568785</td>\n",
       "      <td>@RepTedYoho @FLGovScott @RonDeSantisFL Now tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tue Jan 01 01:55:10 +0000 2019</td>\n",
       "      <td>-63.045461 18.022863</td>\n",
       "      <td>Impossible blues. #caribbean  #sintmaarten #Du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sat Dec 29 13:59:29 +0000 2018</td>\n",
       "      <td>-81.124037 24.706423</td>\n",
       "      <td>Island Lost. Small Key destroyed from Hurrican...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fri Dec 28 22:26:19 +0000 2018</td>\n",
       "      <td>-63.057474 18.100598</td>\n",
       "      <td>In between today’s raindrops #hurricaneirma #r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fri Dec 28 17:51:28 +0000 2018</td>\n",
       "      <td>-90.383100 38.727200</td>\n",
       "      <td>Still the funniest shit ever lReposted from @b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Thu Dec 27 22:00:32 +0000 2018</td>\n",
       "      <td>-63.094055 18.032887</td>\n",
       "      <td>A full day in the Dutch/French island in the w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sat Dec 08 22:45:02 +0000 2018</td>\n",
       "      <td>-62.827700 17.897700</td>\n",
       "      <td>Eden Rock Hotel might still be reinventing its...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Thu Sep 27 17:19:09 +0000 2018</td>\n",
       "      <td>-83.174198 30.660870</td>\n",
       "      <td>#TBT Throwing it back to the beautiful wedding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mon Sep 17 16:24:06 +0000 2018</td>\n",
       "      <td>-81.875298 26.636909</td>\n",
       "      <td>I’m thinking of the people in #northcarolina w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Fri Sep 14 22:51:46 +0000 2018</td>\n",
       "      <td>-66.113500 18.464783</td>\n",
       "      <td>A doorway to #PuertoRico above the Paseo del M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Fri Sep 14 22:51:44 +0000 2018</td>\n",
       "      <td>-66.113500 18.464783</td>\n",
       "      <td>A #PuertoRico street above the Paseo del Morro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Fri Sep 14 14:05:14 +0000 2018</td>\n",
       "      <td>-66.113500 18.464783</td>\n",
       "      <td>Lobster traps in #PuertoRico near the Paseo de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Fri Sep 14 13:56:54 +0000 2018</td>\n",
       "      <td>-66.066700 18.450000</td>\n",
       "      <td>Raices Fountain in #oldSanJuan. #PuertoRico is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Fri Sep 14 00:32:58 +0000 2018</td>\n",
       "      <td>-66.113500 18.464783</td>\n",
       "      <td>This is the “Crecimiento Monument.” It is grow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Wed Sep 12 00:33:10 +0000 2018</td>\n",
       "      <td>-64.922004 18.320629</td>\n",
       "      <td>My experience as #HurricaneIrma hit that morni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Tue Sep 11 12:37:44 +0000 2018</td>\n",
       "      <td>-80.192000 25.775200</td>\n",
       "      <td>#OnThisDate2017📰 #HurricaneIrma🌀 @ Miami, Flor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Tue Sep 11 02:34:51 +0000 2018</td>\n",
       "      <td>-81.468600 26.335600</td>\n",
       "      <td>A year ago today... #hurricaneirma #lifeinnapl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Mon Sep 10 19:00:59 +0000 2018</td>\n",
       "      <td>-81.728900 26.221200</td>\n",
       "      <td>One year ago today, things got kind of stormy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Mon Sep 10 13:13:14 +0000 2018</td>\n",
       "      <td>-81.542960 28.350480</td>\n",
       "      <td>A year ago I was in #disneyworld stuck in my r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Mon Sep 10 08:48:50 +0000 2018</td>\n",
       "      <td>-81.482300 24.664300</td>\n",
       "      <td>A big honkin’ #hurricane made #landfall in #Fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Sun Sep 09 23:28:22 +0000 2018</td>\n",
       "      <td>-81.800000 24.550000</td>\n",
       "      <td>Happy 1st Birthday, Irma Blue! Thank you for a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Sun Sep 09 16:39:23 +0000 2018</td>\n",
       "      <td>-66.500000 18.250000</td>\n",
       "      <td>Bienvenidos a Puerto Rico! #PuertoRican #fundr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Sun Sep 09 08:08:08 +0000 2018</td>\n",
       "      <td>-66.065432 18.396465</td>\n",
       "      <td>Bienvendos a Puerto Rico! #PuertoRican #fundra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Fri Sep 07 22:25:31 +0000 2018</td>\n",
       "      <td>-81.801934 26.132012</td>\n",
       "      <td>#Flashbackfriday to this time last year: we we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Fri Sep 07 21:34:21 +0000 2018</td>\n",
       "      <td>-81.060003 28.755946</td>\n",
       "      <td>It's a #redneck #fridaynight. I'd like to say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Fri Sep 07 14:37:05 +0000 2018</td>\n",
       "      <td>-66.061042 18.387527</td>\n",
       "      <td>Remember #GivingTuesdayPR in November. roberto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Fri Sep 07 14:36:07 +0000 2018</td>\n",
       "      <td>-66.061042 18.387527</td>\n",
       "      <td>Remember #GivingTuesdayPR in November. roberto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Fri Sep 07 03:52:00 +0000 2018</td>\n",
       "      <td>-77.036700 38.895100</td>\n",
       "      <td>Today Reppin it for our home #Tortola #BVI &amp;am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Fri Sep 07 02:36:50 +0000 2018</td>\n",
       "      <td>-80.239639 25.728479</td>\n",
       "      <td>#tbt Sept 10, 2017 Hurricane Irma’s Aftermath....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3920</th>\n",
       "      <td>Mon Sep 04 22:25:07 +0000 2017</td>\n",
       "      <td>-64.922004 18.320629</td>\n",
       "      <td>Last #sunset before #hurricaneirma #stthomas #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3921</th>\n",
       "      <td>Mon Sep 04 22:20:48 +0000 2017</td>\n",
       "      <td>-80.336561 25.633914</td>\n",
       "      <td>I was lucky to get even this. #HurricaneIrma #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3922</th>\n",
       "      <td>Mon Sep 04 21:52:18 +0000 2017</td>\n",
       "      <td>-80.076400 26.528100</td>\n",
       "      <td>People trying to fill up before #hurricaneirma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3923</th>\n",
       "      <td>Mon Sep 04 21:45:04 +0000 2017</td>\n",
       "      <td>-80.313700 26.012900</td>\n",
       "      <td>Not taking any chances. #hurricaneirma #youren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3924</th>\n",
       "      <td>Mon Sep 04 21:20:17 +0000 2017</td>\n",
       "      <td>-78.076100 43.335800</td>\n",
       "      <td>#hurricaneirma #licensedpublicadjuster #proper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3925</th>\n",
       "      <td>Mon Sep 04 21:10:38 +0000 2017</td>\n",
       "      <td>-81.410000 29.910000</td>\n",
       "      <td>Brace yourself. 🙈 #hurricaneirma #floridalivin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3926</th>\n",
       "      <td>Mon Sep 04 20:39:23 +0000 2017</td>\n",
       "      <td>-73.809137 40.671411</td>\n",
       "      <td>@NHC_Atlantic has upgraded #HurricaneIrma to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3927</th>\n",
       "      <td>Mon Sep 04 20:38:10 +0000 2017</td>\n",
       "      <td>-73.808800 40.670663</td>\n",
       "      <td>A good look at #HurricaneIrma on @flightradar2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3928</th>\n",
       "      <td>Mon Sep 04 19:34:32 +0000 2017</td>\n",
       "      <td>-81.800000 24.550000</td>\n",
       "      <td>It looks like #HurricaneIrma is on her way! Ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3929</th>\n",
       "      <td>Mon Sep 04 19:26:05 +0000 2017</td>\n",
       "      <td>-81.564384 28.454373</td>\n",
       "      <td>With #hurricaneirma on the way, we are saving ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3930</th>\n",
       "      <td>Mon Sep 04 19:24:43 +0000 2017</td>\n",
       "      <td>-84.361552 33.825987</td>\n",
       "      <td>According to #Ventusky website, this is where ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3931</th>\n",
       "      <td>Mon Sep 04 19:13:51 +0000 2017</td>\n",
       "      <td>-81.961219 30.123032</td>\n",
       "      <td>And there you have it folks- (always be prepar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3932</th>\n",
       "      <td>Mon Sep 04 19:12:29 +0000 2017</td>\n",
       "      <td>-81.626669 28.468916</td>\n",
       "      <td>#HurricaneIrma is at least five days away, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3933</th>\n",
       "      <td>Tue Sep 05 06:08:21 +0000 2017</td>\n",
       "      <td>-80.224100 25.787700</td>\n",
       "      <td>Heads Up #HurricaneIrma is approaching #Miami ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3934</th>\n",
       "      <td>Tue Sep 05 06:06:56 +0000 2017</td>\n",
       "      <td>-80.224100 25.787700</td>\n",
       "      <td>Heads Up #HurricaneIrma is approaching #Miami ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3935</th>\n",
       "      <td>Tue Sep 05 05:57:06 +0000 2017</td>\n",
       "      <td>18.397411 54.790779</td>\n",
       "      <td>La Calma antes de la Tormenta #puertorico #cal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3936</th>\n",
       "      <td>Tue Sep 05 05:25:56 +0000 2017</td>\n",
       "      <td>-80.138740 26.156550</td>\n",
       "      <td>#tothemoon #tothemooncandy #irma #hurricaneirm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3937</th>\n",
       "      <td>Tue Sep 05 04:09:20 +0000 2017</td>\n",
       "      <td>-80.269279 25.789734</td>\n",
       "      <td>Smart yacht owners have their captains take th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3938</th>\n",
       "      <td>Tue Sep 05 03:50:25 +0000 2017</td>\n",
       "      <td>-78.824700 35.654400</td>\n",
       "      <td>For #hurricaneirma #hurricanharvey Prayer Warr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3939</th>\n",
       "      <td>Tue Sep 05 03:24:17 +0000 2017</td>\n",
       "      <td>-80.047814 26.614863</td>\n",
       "      <td>#hurricaneirma #florida #catagory4 #fuckyouhur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3940</th>\n",
       "      <td>Tue Sep 05 03:03:09 +0000 2017</td>\n",
       "      <td>-81.503333 28.658333</td>\n",
       "      <td>#hurricaneirma #itsgonnagetus #onewayoranother...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3941</th>\n",
       "      <td>Tue Sep 05 02:32:44 +0000 2017</td>\n",
       "      <td>-80.224100 25.787700</td>\n",
       "      <td>Preparing for Hurricane Irma. Fun times ☹️😠 #h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3942</th>\n",
       "      <td>Tue Sep 05 02:18:44 +0000 2017</td>\n",
       "      <td>-82.209956 42.408139</td>\n",
       "      <td>Oh oh… looks like Key West is in the cone agai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3943</th>\n",
       "      <td>Tue Sep 05 02:14:06 +0000 2017</td>\n",
       "      <td>-80.254100 26.655100</td>\n",
       "      <td>My cat says #hurricaneirma what?? Sometimes I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3944</th>\n",
       "      <td>Tue Sep 05 01:47:59 +0000 2017</td>\n",
       "      <td>-80.354965 27.275029</td>\n",
       "      <td>Hmmm.... I never know what water to get?!?!?!?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>Tue Sep 05 01:35:59 +0000 2017</td>\n",
       "      <td>-95.515000 29.925300</td>\n",
       "      <td>#irma #hurricaneirma #atlanta famerica_kennyma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>Tue Sep 05 01:08:30 +0000 2017</td>\n",
       "      <td>-64.617484 18.423466</td>\n",
       "      <td>Enjoying the pool before hurricane Irma. #hurr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3947</th>\n",
       "      <td>Tue Sep 05 00:59:51 +0000 2017</td>\n",
       "      <td>-80.130187 26.097175</td>\n",
       "      <td>In light of #HurricaneIrma, all florida reside...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3948</th>\n",
       "      <td>Tue Sep 05 00:57:20 +0000 2017</td>\n",
       "      <td>-64.929540 18.343080</td>\n",
       "      <td>Stay safe everyone! #hurricaneirma All closed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3949</th>\n",
       "      <td>Tue Sep 05 00:53:06 +0000 2017</td>\n",
       "      <td>-80.326473 25.621800</td>\n",
       "      <td>Northsplainin': When friends from non-hurrican...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3950 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                date           coordinates  \\\n",
       "0     Thu Jan 10 18:37:23 +0000 2019  -80.192000 25.775200   \n",
       "1     Wed Jan 02 15:13:57 +0000 2019  -81.592912 29.568785   \n",
       "2     Tue Jan 01 01:55:10 +0000 2019  -63.045461 18.022863   \n",
       "3     Sat Dec 29 13:59:29 +0000 2018  -81.124037 24.706423   \n",
       "4     Fri Dec 28 22:26:19 +0000 2018  -63.057474 18.100598   \n",
       "5     Fri Dec 28 17:51:28 +0000 2018  -90.383100 38.727200   \n",
       "6     Thu Dec 27 22:00:32 +0000 2018  -63.094055 18.032887   \n",
       "7     Sat Dec 08 22:45:02 +0000 2018  -62.827700 17.897700   \n",
       "8     Thu Sep 27 17:19:09 +0000 2018  -83.174198 30.660870   \n",
       "9     Mon Sep 17 16:24:06 +0000 2018  -81.875298 26.636909   \n",
       "10    Fri Sep 14 22:51:46 +0000 2018  -66.113500 18.464783   \n",
       "11    Fri Sep 14 22:51:44 +0000 2018  -66.113500 18.464783   \n",
       "12    Fri Sep 14 14:05:14 +0000 2018  -66.113500 18.464783   \n",
       "13    Fri Sep 14 13:56:54 +0000 2018  -66.066700 18.450000   \n",
       "14    Fri Sep 14 00:32:58 +0000 2018  -66.113500 18.464783   \n",
       "15    Wed Sep 12 00:33:10 +0000 2018  -64.922004 18.320629   \n",
       "16    Tue Sep 11 12:37:44 +0000 2018  -80.192000 25.775200   \n",
       "17    Tue Sep 11 02:34:51 +0000 2018  -81.468600 26.335600   \n",
       "18    Mon Sep 10 19:00:59 +0000 2018  -81.728900 26.221200   \n",
       "19    Mon Sep 10 13:13:14 +0000 2018  -81.542960 28.350480   \n",
       "20    Mon Sep 10 08:48:50 +0000 2018  -81.482300 24.664300   \n",
       "21    Sun Sep 09 23:28:22 +0000 2018  -81.800000 24.550000   \n",
       "22    Sun Sep 09 16:39:23 +0000 2018  -66.500000 18.250000   \n",
       "23    Sun Sep 09 08:08:08 +0000 2018  -66.065432 18.396465   \n",
       "24    Fri Sep 07 22:25:31 +0000 2018  -81.801934 26.132012   \n",
       "25    Fri Sep 07 21:34:21 +0000 2018  -81.060003 28.755946   \n",
       "26    Fri Sep 07 14:37:05 +0000 2018  -66.061042 18.387527   \n",
       "27    Fri Sep 07 14:36:07 +0000 2018  -66.061042 18.387527   \n",
       "28    Fri Sep 07 03:52:00 +0000 2018  -77.036700 38.895100   \n",
       "29    Fri Sep 07 02:36:50 +0000 2018  -80.239639 25.728479   \n",
       "...                              ...                   ...   \n",
       "3920  Mon Sep 04 22:25:07 +0000 2017  -64.922004 18.320629   \n",
       "3921  Mon Sep 04 22:20:48 +0000 2017  -80.336561 25.633914   \n",
       "3922  Mon Sep 04 21:52:18 +0000 2017  -80.076400 26.528100   \n",
       "3923  Mon Sep 04 21:45:04 +0000 2017  -80.313700 26.012900   \n",
       "3924  Mon Sep 04 21:20:17 +0000 2017  -78.076100 43.335800   \n",
       "3925  Mon Sep 04 21:10:38 +0000 2017  -81.410000 29.910000   \n",
       "3926  Mon Sep 04 20:39:23 +0000 2017  -73.809137 40.671411   \n",
       "3927  Mon Sep 04 20:38:10 +0000 2017  -73.808800 40.670663   \n",
       "3928  Mon Sep 04 19:34:32 +0000 2017  -81.800000 24.550000   \n",
       "3929  Mon Sep 04 19:26:05 +0000 2017  -81.564384 28.454373   \n",
       "3930  Mon Sep 04 19:24:43 +0000 2017  -84.361552 33.825987   \n",
       "3931  Mon Sep 04 19:13:51 +0000 2017  -81.961219 30.123032   \n",
       "3932  Mon Sep 04 19:12:29 +0000 2017  -81.626669 28.468916   \n",
       "3933  Tue Sep 05 06:08:21 +0000 2017  -80.224100 25.787700   \n",
       "3934  Tue Sep 05 06:06:56 +0000 2017  -80.224100 25.787700   \n",
       "3935  Tue Sep 05 05:57:06 +0000 2017   18.397411 54.790779   \n",
       "3936  Tue Sep 05 05:25:56 +0000 2017  -80.138740 26.156550   \n",
       "3937  Tue Sep 05 04:09:20 +0000 2017  -80.269279 25.789734   \n",
       "3938  Tue Sep 05 03:50:25 +0000 2017  -78.824700 35.654400   \n",
       "3939  Tue Sep 05 03:24:17 +0000 2017  -80.047814 26.614863   \n",
       "3940  Tue Sep 05 03:03:09 +0000 2017  -81.503333 28.658333   \n",
       "3941  Tue Sep 05 02:32:44 +0000 2017  -80.224100 25.787700   \n",
       "3942  Tue Sep 05 02:18:44 +0000 2017  -82.209956 42.408139   \n",
       "3943  Tue Sep 05 02:14:06 +0000 2017  -80.254100 26.655100   \n",
       "3944  Tue Sep 05 01:47:59 +0000 2017  -80.354965 27.275029   \n",
       "3945  Tue Sep 05 01:35:59 +0000 2017  -95.515000 29.925300   \n",
       "3946  Tue Sep 05 01:08:30 +0000 2017  -64.617484 18.423466   \n",
       "3947  Tue Sep 05 00:59:51 +0000 2017  -80.130187 26.097175   \n",
       "3948  Tue Sep 05 00:57:20 +0000 2017  -64.929540 18.343080   \n",
       "3949  Tue Sep 05 00:53:06 +0000 2017  -80.326473 25.621800   \n",
       "\n",
       "                                                   text  \n",
       "0     #publicadjuster #wefightforyou #hurricaneirma ...  \n",
       "1     @RepTedYoho @FLGovScott @RonDeSantisFL Now tha...  \n",
       "2     Impossible blues. #caribbean  #sintmaarten #Du...  \n",
       "3     Island Lost. Small Key destroyed from Hurrican...  \n",
       "4     In between today’s raindrops #hurricaneirma #r...  \n",
       "5     Still the funniest shit ever lReposted from @b...  \n",
       "6     A full day in the Dutch/French island in the w...  \n",
       "7     Eden Rock Hotel might still be reinventing its...  \n",
       "8     #TBT Throwing it back to the beautiful wedding...  \n",
       "9     I’m thinking of the people in #northcarolina w...  \n",
       "10    A doorway to #PuertoRico above the Paseo del M...  \n",
       "11    A #PuertoRico street above the Paseo del Morro...  \n",
       "12    Lobster traps in #PuertoRico near the Paseo de...  \n",
       "13    Raices Fountain in #oldSanJuan. #PuertoRico is...  \n",
       "14    This is the “Crecimiento Monument.” It is grow...  \n",
       "15    My experience as #HurricaneIrma hit that morni...  \n",
       "16    #OnThisDate2017📰 #HurricaneIrma🌀 @ Miami, Flor...  \n",
       "17    A year ago today... #hurricaneirma #lifeinnapl...  \n",
       "18    One year ago today, things got kind of stormy ...  \n",
       "19    A year ago I was in #disneyworld stuck in my r...  \n",
       "20    A big honkin’ #hurricane made #landfall in #Fl...  \n",
       "21    Happy 1st Birthday, Irma Blue! Thank you for a...  \n",
       "22    Bienvenidos a Puerto Rico! #PuertoRican #fundr...  \n",
       "23    Bienvendos a Puerto Rico! #PuertoRican #fundra...  \n",
       "24    #Flashbackfriday to this time last year: we we...  \n",
       "25    It's a #redneck #fridaynight. I'd like to say ...  \n",
       "26    Remember #GivingTuesdayPR in November. roberto...  \n",
       "27    Remember #GivingTuesdayPR in November. roberto...  \n",
       "28    Today Reppin it for our home #Tortola #BVI &am...  \n",
       "29    #tbt Sept 10, 2017 Hurricane Irma’s Aftermath....  \n",
       "...                                                 ...  \n",
       "3920  Last #sunset before #hurricaneirma #stthomas #...  \n",
       "3921  I was lucky to get even this. #HurricaneIrma #...  \n",
       "3922  People trying to fill up before #hurricaneirma...  \n",
       "3923  Not taking any chances. #hurricaneirma #youren...  \n",
       "3924  #hurricaneirma #licensedpublicadjuster #proper...  \n",
       "3925  Brace yourself. 🙈 #hurricaneirma #floridalivin...  \n",
       "3926  @NHC_Atlantic has upgraded #HurricaneIrma to a...  \n",
       "3927  A good look at #HurricaneIrma on @flightradar2...  \n",
       "3928  It looks like #HurricaneIrma is on her way! Ar...  \n",
       "3929  With #hurricaneirma on the way, we are saving ...  \n",
       "3930  According to #Ventusky website, this is where ...  \n",
       "3931  And there you have it folks- (always be prepar...  \n",
       "3932  #HurricaneIrma is at least five days away, and...  \n",
       "3933  Heads Up #HurricaneIrma is approaching #Miami ...  \n",
       "3934  Heads Up #HurricaneIrma is approaching #Miami ...  \n",
       "3935  La Calma antes de la Tormenta #puertorico #cal...  \n",
       "3936  #tothemoon #tothemooncandy #irma #hurricaneirm...  \n",
       "3937  Smart yacht owners have their captains take th...  \n",
       "3938  For #hurricaneirma #hurricanharvey Prayer Warr...  \n",
       "3939  #hurricaneirma #florida #catagory4 #fuckyouhur...  \n",
       "3940  #hurricaneirma #itsgonnagetus #onewayoranother...  \n",
       "3941  Preparing for Hurricane Irma. Fun times ☹️😠 #h...  \n",
       "3942  Oh oh… looks like Key West is in the cone agai...  \n",
       "3943  My cat says #hurricaneirma what?? Sometimes I ...  \n",
       "3944  Hmmm.... I never know what water to get?!?!?!?...  \n",
       "3945  #irma #hurricaneirma #atlanta famerica_kennyma...  \n",
       "3946  Enjoying the pool before hurricane Irma. #hurr...  \n",
       "3947  In light of #HurricaneIrma, all florida reside...  \n",
       "3948  Stay safe everyone! #hurricaneirma All closed ...  \n",
       "3949  Northsplainin': When friends from non-hurrican...  \n",
       "\n",
       "[3950 rows x 3 columns]"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final.to_csv('C:\\\\Ashish\\\\Project\\\\dataset\\\\irma-coordinates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "major_project",
   "language": "python",
   "name": "major_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
