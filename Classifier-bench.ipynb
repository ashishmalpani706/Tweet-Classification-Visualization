{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    weather another video anderson cooper hype flo...\n",
       "1                         star hurricane florence nasa\n",
       "2    hurricane florence still move atlantic morning...\n",
       "3    houston collect donation hurricane florence vi...\n",
       "4    still collect supply victim hurricane florence...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "df = pd.read_csv('C:\\\\Ashish\\\\Project\\\\dataset\\\\clean-trainset1.csv')#,nrows = 1500)\n",
    "df.text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "def train(classifier, X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=33)\n",
    " \n",
    "    classifier.fit(X_train, y_train)\n",
    "    print(\"Accuracy: %s\" % classifier.score(X_test, y_test))\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6789366053169734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=...      vocabulary=None)), ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " \n",
    "trial1 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', MultinomialNB()),\n",
    "])\n",
    " \n",
    "train(trial1, df.text, df.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7096114519427403\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=...     vocabulary=None)), ('classifier', MultinomialNB(alpha=0.05, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    " \n",
    "trial2 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', MultinomialNB(alpha=0.05)),\n",
    "])\n",
    " \n",
    "train(trial2, df.text, df.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7321063394683026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=...     vocabulary=None)), ('classifier', MultinomialNB(alpha=0.05, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial3 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english'),\n",
    "                             min_df=5)),\n",
    "    ('classifier', MultinomialNB(alpha=0.05)),\n",
    "])\n",
    " \n",
    "train(trial3, df.text, df.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incremental_range(start, stop, inc):\n",
    "    value = start\n",
    "    while value < stop:\n",
    "        yield value\n",
    "        value += inc\n",
    "\n",
    "weights = list(incremental_range(0, 1, 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7137014314928425\n",
      "Accuracy: 0.7321063394683026\n",
      "Accuracy: 0.7321063394683026"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ashis\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.7321063394683026\n",
      "Accuracy: 0.7300613496932515\n",
      "Accuracy: 0.7321063394683026\n",
      "Accuracy: 0.7341513292433538\n",
      "Accuracy: 0.7300613496932515\n",
      "Accuracy: 0.7259713701431493\n",
      "Accuracy: 0.7280163599182005\n",
      "Accuracy: 0.7280163599182005\n",
      "Accuracy: 0.7239263803680982\n",
      "Accuracy: 0.721881390593047\n",
      "Accuracy: 0.7280163599182005\n",
      "Accuracy: 0.7300613496932515\n",
      "Accuracy: 0.7259713701431493\n",
      "Accuracy: 0.7259713701431493\n",
      "Accuracy: 0.7259713701431493\n",
      "Accuracy: 0.7239263803680982\n",
      "Accuracy: 0.721881390593047\n"
     ]
    }
   ],
   "source": [
    "for x in weights:\n",
    "    trialx = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english'),\n",
    "                             min_df=5)),\n",
    "    ('classifier', MultinomialNB(alpha=x)),\n",
    "    ])\n",
    "    train(trialx, df.text, df.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6768916155419223\n",
      "Accuracy: 0.7137014314928425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ashis\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7034764826175869\n",
      "Accuracy: 0.7075664621676891\n",
      "Accuracy: 0.7075664621676891\n",
      "Accuracy: 0.7096114519427403\n",
      "Accuracy: 0.7177914110429447\n",
      "Accuracy: 0.7075664621676891\n",
      "Accuracy: 0.7034764826175869\n",
      "Accuracy: 0.7034764826175869\n",
      "Accuracy: 0.6973415132924335\n",
      "Accuracy: 0.6993865030674846\n",
      "Accuracy: 0.7034764826175869\n",
      "Accuracy: 0.7014314928425358\n",
      "Accuracy: 0.6993865030674846\n",
      "Accuracy: 0.6912065439672802\n",
      "Accuracy: 0.6912065439672802\n",
      "Accuracy: 0.689161554192229\n",
      "Accuracy: 0.689161554192229\n",
      "Accuracy: 0.6768916155419223\n",
      "Accuracy: 0.6768916155419223\n",
      "Accuracy: 0.6973415132924335\n",
      "Accuracy: 0.6952965235173824\n",
      "Accuracy: 0.7014314928425358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ashis\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7034764826175869\n",
      "Accuracy: 0.7034764826175869\n",
      "Accuracy: 0.7075664621676891\n",
      "Accuracy: 0.7116564417177914\n",
      "Accuracy: 0.7137014314928425\n",
      "Accuracy: 0.7137014314928425\n",
      "Accuracy: 0.7177914110429447\n",
      "Accuracy: 0.721881390593047\n",
      "Accuracy: 0.7239263803680982\n",
      "Accuracy: 0.7116564417177914\n",
      "Accuracy: 0.7137014314928425\n",
      "Accuracy: 0.7137014314928425\n",
      "Accuracy: 0.7137014314928425\n",
      "Accuracy: 0.7116564417177914\n",
      "Accuracy: 0.6952965235173824\n",
      "Accuracy: 0.6952965235173824\n",
      "Accuracy: 0.6728016359918201\n",
      "Accuracy: 0.6850715746421268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ashis\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6871165644171779\n",
      "Accuracy: 0.6912065439672802\n",
      "Accuracy: 0.6932515337423313\n",
      "Accuracy: 0.6993865030674846\n",
      "Accuracy: 0.7034764826175869\n",
      "Accuracy: 0.7034764826175869\n",
      "Accuracy: 0.7014314928425358\n",
      "Accuracy: 0.7034764826175869\n",
      "Accuracy: 0.7014314928425358\n",
      "Accuracy: 0.7116564417177914\n",
      "Accuracy: 0.7116564417177914\n",
      "Accuracy: 0.7137014314928425\n",
      "Accuracy: 0.7137014314928425\n",
      "Accuracy: 0.7096114519427403\n",
      "Accuracy: 0.7096114519427403\n",
      "Accuracy: 0.7116564417177914\n",
      "Accuracy: 0.7034764826175869\n",
      "Accuracy: 0.7034764826175869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ashis\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7096114519427403\n",
      "Accuracy: 0.7239263803680982\n",
      "Accuracy: 0.7259713701431493\n",
      "Accuracy: 0.7259713701431493\n",
      "Accuracy: 0.7239263803680982\n",
      "Accuracy: 0.7239263803680982\n",
      "Accuracy: 0.721881390593047\n",
      "Accuracy: 0.7198364008179959\n",
      "Accuracy: 0.721881390593047\n",
      "Accuracy: 0.721881390593047\n",
      "Accuracy: 0.721881390593047\n",
      "Accuracy: 0.7280163599182005\n",
      "Accuracy: 0.7239263803680982\n",
      "Accuracy: 0.7259713701431493\n",
      "Accuracy: 0.7198364008179959\n",
      "Accuracy: 0.7259713701431493\n",
      "Accuracy: 0.7239263803680982\n",
      "Accuracy: 0.7239263803680982\n",
      "Accuracy: 0.721881390593047\n",
      "Accuracy: 0.721881390593047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ashis\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7137014314928425\n",
      "Accuracy: 0.7321063394683026\n",
      "Accuracy: 0.7321063394683026\n",
      "Accuracy: 0.7321063394683026\n",
      "Accuracy: 0.7300613496932515\n",
      "Accuracy: 0.7321063394683026\n",
      "Accuracy: 0.7341513292433538\n",
      "Accuracy: 0.7300613496932515\n",
      "Accuracy: 0.7259713701431493\n",
      "Accuracy: 0.7280163599182005\n",
      "Accuracy: 0.7280163599182005\n",
      "Accuracy: 0.7239263803680982\n",
      "Accuracy: 0.721881390593047\n",
      "Accuracy: 0.7280163599182005\n",
      "Accuracy: 0.7300613496932515\n",
      "Accuracy: 0.7259713701431493\n",
      "Accuracy: 0.7259713701431493\n",
      "Accuracy: 0.7259713701431493\n",
      "Accuracy: 0.7239263803680982\n",
      "Accuracy: 0.721881390593047\n",
      "Accuracy: 0.7116564417177914\n",
      "Accuracy: 0.7177914110429447\n",
      "Accuracy: 0.7157464212678937"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ashis\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.7198364008179959\n",
      "Accuracy: 0.7198364008179959\n",
      "Accuracy: 0.7177914110429447\n",
      "Accuracy: 0.7239263803680982\n",
      "Accuracy: 0.7259713701431493\n",
      "Accuracy: 0.7280163599182005\n",
      "Accuracy: 0.7280163599182005\n",
      "Accuracy: 0.7239263803680982\n",
      "Accuracy: 0.7198364008179959\n",
      "Accuracy: 0.7177914110429447\n",
      "Accuracy: 0.7177914110429447\n",
      "Accuracy: 0.7198364008179959\n",
      "Accuracy: 0.7198364008179959\n",
      "Accuracy: 0.7177914110429447\n",
      "Accuracy: 0.7198364008179959\n",
      "Accuracy: 0.7198364008179959\n",
      "Accuracy: 0.7177914110429447\n",
      "Accuracy: 0.6952965235173824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ashis\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7034764826175869\n",
      "Accuracy: 0.6993865030674846\n",
      "Accuracy: 0.7014314928425358\n",
      "Accuracy: 0.7034764826175869\n",
      "Accuracy: 0.7055214723926381\n",
      "Accuracy: 0.7034764826175869\n",
      "Accuracy: 0.7014314928425358\n",
      "Accuracy: 0.7034764826175869\n",
      "Accuracy: 0.7055214723926381\n",
      "Accuracy: 0.7096114519427403\n",
      "Accuracy: 0.7075664621676891\n",
      "Accuracy: 0.7075664621676891\n",
      "Accuracy: 0.7055214723926381\n",
      "Accuracy: 0.7055214723926381\n",
      "Accuracy: 0.7034764826175869\n",
      "Accuracy: 0.7014314928425358\n",
      "Accuracy: 0.7014314928425358\n",
      "Accuracy: 0.6973415132924335\n",
      "Accuracy: 0.6973415132924335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ashis\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6871165644171779\n",
      "Accuracy: 0.6973415132924335\n",
      "Accuracy: 0.689161554192229\n",
      "Accuracy: 0.689161554192229\n",
      "Accuracy: 0.689161554192229\n",
      "Accuracy: 0.689161554192229\n",
      "Accuracy: 0.6871165644171779\n",
      "Accuracy: 0.689161554192229\n",
      "Accuracy: 0.6912065439672802\n",
      "Accuracy: 0.6871165644171779\n",
      "Accuracy: 0.6830265848670757\n",
      "Accuracy: 0.6850715746421268\n",
      "Accuracy: 0.6871165644171779\n",
      "Accuracy: 0.689161554192229\n",
      "Accuracy: 0.6932515337423313\n",
      "Accuracy: 0.6932515337423313\n",
      "Accuracy: 0.6952965235173824\n",
      "Accuracy: 0.689161554192229\n",
      "Accuracy: 0.6871165644171779\n",
      "Accuracy: 0.6871165644171779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ashis\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\naive_bayes.py:480: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6830265848670757\n",
      "Accuracy: 0.6830265848670757\n",
      "Accuracy: 0.6830265848670757\n",
      "Accuracy: 0.6809815950920245\n",
      "Accuracy: 0.6809815950920245\n",
      "Accuracy: 0.6789366053169734\n",
      "Accuracy: 0.6768916155419223\n",
      "Accuracy: 0.6789366053169734\n",
      "Accuracy: 0.6789366053169734\n",
      "Accuracy: 0.6768916155419223\n",
      "Accuracy: 0.6748466257668712\n",
      "Accuracy: 0.6707566462167689\n",
      "Accuracy: 0.6748466257668712\n",
      "Accuracy: 0.6768916155419223\n",
      "Accuracy: 0.6789366053169734\n",
      "Accuracy: 0.6809815950920245\n",
      "Accuracy: 0.6789366053169734\n",
      "Accuracy: 0.6728016359918201\n",
      "Accuracy: 0.6728016359918201\n",
      "Accuracy: 0.6728016359918201\n"
     ]
    }
   ],
   "source": [
    "for num in range(1,10,1):\n",
    "    for x in weights:\n",
    "        trialx = Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english'),\n",
    "                                 min_df=num)),\n",
    "        ('classifier', MultinomialNB(alpha=x)),\n",
    "        ])\n",
    "        train(trialx, df.text, df.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7341513292433538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=...      vocabulary=None)), ('classifier', MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial3 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english'),\n",
    "                             min_df=5,norm='l2')),\n",
    "    ('classifier', MultinomialNB(alpha=0.30)),\n",
    "])\n",
    " \n",
    "train(trial3, df.text, df.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import string\n",
    "# from nltk.stem import PorterStemmer\n",
    "# from nltk import word_tokenize\n",
    " \n",
    "# def stemming_tokenizer(text):\n",
    "#     stemmer = PorterStemmer()\n",
    "#     return [stemmer.stem(w) for w in word_tokenize(text)]\n",
    " \n",
    "# trial5 = Pipeline([\n",
    "#     ('vectorizer', TfidfVectorizer(min_df=5, tokenizer=stemming_tokenizer,\n",
    "#                              stop_words=stopwords.words('english'))),\n",
    "#     ('classifier', MultinomialNB(alpha=0.30)),\n",
    "# ])\n",
    " \n",
    "# train(trial5, df.text, df.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7014314928425358\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "trial4 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english'),\n",
    "                             min_df=1)),\n",
    "    ('classifier', LinearSVC()),\n",
    "])\n",
    " \n",
    "train(trial4, df.text, df.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7014314928425358\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=2,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=...penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "trial5 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english'),\n",
    "                             min_df=2)),\n",
    "    ('classifier', LogisticRegression()),\n",
    "])\n",
    " \n",
    "train(trial5, df.text, df.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7341513292433538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=...      vocabulary=None)), ('classifier', MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Winner\n",
    "train(trial3, df.text, df.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "def train(classifier, X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=33)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    print(\"Accuracy: %s\" % classifier.score(X_test, y_test))\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english'),\n",
    "                             min_df=5,norm='l2')),\n",
    "    ('classifier', MultinomialNB(alpha=0.30)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    #weather Another #fakenews video @CNN Anderson...\n",
       "1    Staring Down Hurricane Florence via NASA https...\n",
       "2    #hurricane #florence http:// tra.one/tcphurFlo...\n",
       "3    Hurricane Florence is still moving through the...\n",
       "4    Houston is collecting donations for Hurricane ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "df = pd.read_csv('C:\\\\Ashish\\\\Project\\\\dataset\\\\trainset1.csv')#,nrows = 1500)\n",
    "df.text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Accuracy: 0.6541501976284585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=...      vocabulary=None)), ('classifier', MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(trial, df.text, df.category) #No pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "    return input_txt  \n",
    "\n",
    "# remove twitter handles (@user)\n",
    "df['text'] = np.vectorize(remove_pattern)(df['text'], \"@[\\w]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6600790513833992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=...      vocabulary=None)), ('classifier', MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(trial, df.text, df.category) #No pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO - REMOVE ASCII EXTENDED\n",
    "for i in range(len(df)):\n",
    "    tweet = df.iloc[i,df.columns.get_loc('text')]\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r\"\\S+\\/.\\S+ *\\S+|.\\S+html|\\S+-\\S+|\\d*\\/\\d+|\\d+|\\S+%\\S+|\\S+:\\S*|\\S+=\\S+|.#\\S+\", \"\", tweet)\n",
    "    tweet = tweet.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "    df.iloc[i,df.columns.get_loc('text')] = tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.66600790513834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=...      vocabulary=None)), ('classifier', MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(trial, df.text, df.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "stop_words = set(stopwords.words('english'))\n",
    "for i in range(len(df)):\n",
    "    tweet = df.iloc[i,df.columns.get_loc('text')]\n",
    "    word_tokens = word_tokenize(tweet) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    filtered_sentence = [] \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w) \n",
    "    tweet = ' '.join(filtered_sentence)\n",
    "    df.iloc[i,df.columns.get_loc('text')] = tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.66600790513834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=...      vocabulary=None)), ('classifier', MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(trial, df.text, df.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_words = ['via','rt','fav','…','am','et','pm','n\\'t','y\\'all']\n",
    "for i in range(len(df)):\n",
    "    tweet = df.iloc[i,df.columns.get_loc('text')]\n",
    "    querywords = tweet.split()\n",
    "    resultwords  = [word for word in querywords if word.lower() not in custom_words]\n",
    "    result = ' '.join(resultwords)\n",
    "    df.iloc[i,df.columns.get_loc('text')] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6640316205533597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=...      vocabulary=None)), ('classifier', MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(trial, df.text, df.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "remove = string.punctuation + \".‘’\\''“”°…-—––•・®.:#\"\n",
    "for i in range(len(df)):\n",
    "    tweet = df.iloc[i,df.columns.get_loc('text')]\n",
    "    tweet = ' '.join(word.strip(remove) for word in tweet.split())\n",
    "    tweet = tweet.strip()\n",
    "    df.iloc[i,df.columns.get_loc('text')] = tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6640316205533597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=...      vocabulary=None)), ('classifier', MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(trial, df.text, df.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize with POS Tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# 1. Init Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for i in range(len(df)):\n",
    "    tweet = df.iloc[i,df.columns.get_loc('text')]\n",
    "    tweet =([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(tweet)])\n",
    "    tweet = ' '.join(tweet)\n",
    "    df.iloc[i,df.columns.get_loc('text')] = tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6541501976284585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=...      vocabulary=None)), ('classifier', MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(trial, df.text, df.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REVIEW NEEDED\n",
    "import nltk\n",
    "words = set(nltk.corpus.words.words())\n",
    "for i in range(len(df)):\n",
    "    tweet = df.iloc[i,df.columns.get_loc('text')]\n",
    "    tweet = ' '.join(word for word in tweet.split() if len(word)>3)\n",
    "    df.iloc[i,df.columns.get_loc('text')] = tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6600790513833992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=...      vocabulary=None)), ('classifier', MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(trial, df.text, df.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df['text'].str.split().str.len()\n",
    "df = df[~(count < 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7341513292433538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=...      vocabulary=None)), ('classifier', MultinomialNB(alpha=0.3, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(trial, df.text, df.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-07 23:26:02,581 loading file C:\\Users\\ashis\\.flair\\models\\imdb.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ashis\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\serialization.py:542: DeprecationWarning: Call to deprecated class DocumentLSTMEmbeddings. (The functionality of this class is moved to 'DocumentRNNEmbeddings') -- Deprecated since version 0.4.\n",
      "  result = unpickler.load()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence above is:  [POSITIVE (1.0)]\n"
     ]
    }
   ],
   "source": [
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "\n",
    "classifier = TextClassifier.load('en-sentiment')\n",
    "\n",
    "sentence = Sentence('Flair is pretty neat!')\n",
    "classifier.predict(sentence)\n",
    "\n",
    "# print sentence with predicted labels\n",
    "print('Sentence above is: ', sentence.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"C:\\\\Ashish\\\\Project\\\\dataset\\\\clean-trainset1.csv\", encoding='latin-1').sample(frac=1).drop_duplicates()\n",
    "\n",
    "data = data[['v1', 'v2']].rename(columns={\"v1\":\"label\", \"v2\":\"text\"})\n",
    " \n",
    "data['category'] = '__label__' + data['label'].astype(str)\n",
    "\n",
    "data.iloc[0:int(len(data)*0.8)].to_csv('train.csv', sep='\\t', index = False, header = False)\n",
    "data.iloc[int(len(data)*0.8):int(len(data)*0.9)].to_csv('test.csv', sep='\\t', index = False, header = False)\n",
    "data.iloc[int(len(data)*0.9):].to_csv('dev.csv', sep='\\t', index = False, header = False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "major_project",
   "language": "python",
   "name": "major_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
